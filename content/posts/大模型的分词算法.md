---
{
  "title": "å¤§æ¨¡å‹çš„åˆ†è¯ç®—æ³•",
  "date": "2023-12-06",
  "tags": [
    "tokenizer"
  ],
  "categories": [
    "LLM"
  ],
  "summary": "å¤§æ¨¡å‹å¦‚ä½•è¯»æ‡‚è‡ªç„¶è¯­è¨€ï¼Œéœ€è¦æˆ‘ä»¬å°†æ–‡æœ¬è½¬æˆæœºå™¨èƒ½è¯»æ‡‚çš„æ•°å­—ï¼Œè¿™ç¯‡æ–‡ç« ä»‹ç»äº†ä¸€äº›ä»æ–‡æœ¬åˆ°æ•°å­—çš„ç®—æ³•ã€‚",
  "authors": [],
  "draft": false,
  "generated_time": "2025-02-24T09:43:20.402043"
}
---

çœ‹äº†ä¸€äº›ä»‹ç»tokenizerç®—æ³•çš„æ–‡ç« ï¼Œæ„Ÿè§‰å¤§å®¶ä»‹ç»çš„æ—¶å€™å¹¶ä¸æ˜¯å¾ˆç³»ç»Ÿï¼Œç”šè‡³æœ‰çš„æ–‡ç« è¿˜æœ‰ä¸€äº›é”™è¯¯ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™ç¯‡æ–‡ç« åšäº†ä¸€ä¸ªæ±‡æ€»ï¼Œæ²¡æœ‰ä¸ªäººåŸåˆ›çš„å†…å®¹ã€‚

## BPE

> ğŸ’¡ è¾“å…¥ï¼šè®­ç»ƒè¯­æ–™ï¼› è¯è¡¨å¤§å°V

1.å‡†å¤‡è¶³å¤Ÿå¤§çš„è®­ç»ƒè¯­æ–™ï¼Œç¡®å®šæœŸæœ›çš„subwordè¯è¡¨å¤§å°ï¼›
2.å‡†å¤‡åŸºç¡€è¯è¡¨ï¼šæ¯”å¦‚è‹±æ–‡ä¸­26ä¸ªå­—æ¯åŠ ä¸Šå„ç§ç¬¦å·ï¼›
3.åŸºäºåŸºç¡€è¯è¡¨å°†è¯­æ–™ä¸­çš„å•è¯æ‹†åˆ†ä¸ºå­—ç¬¦åºåˆ—å¹¶åœ¨æœ«å°¾æ·»åŠ åç¼€â€œ </ w>â€ï¼›æœ¬é˜¶æ®µçš„subwordçš„ç²’åº¦æ˜¯å­—ç¬¦ã€‚ä¾‹å¦‚å•è¯â€œ lowâ€çš„é¢‘ç‡ä¸º5ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†å…¶æ”¹å†™ä¸ºâ€œ l o w </ w>â€ï¼š5ï¼›
4.ç»Ÿè®¡æ¯ä¸€ä¸ªè¿ç»­å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡ï¼Œé€‰æ‹©æœ€é«˜é¢‘çš„å­—ç¬¦å¯¹åˆå¹¶æˆæ–°çš„subwordï¼›
5.é‡å¤ç¬¬4æ­¥ç›´åˆ°è¾¾åˆ°ç¬¬1æ­¥è®¾å®šçš„subwordè¯è¡¨å¤§å°æˆ–ä¸‹ä¸€ä¸ªæœ€é«˜é¢‘çš„å­—èŠ‚å¯¹å‡ºç°é¢‘ç‡ä¸º1ï¼›

ä»£ç å‚è€ƒè¿™ç¯‡æ–‡ç« ï¼ŒçŸ¥ä¹å‡ ä¹æ‰€æœ‰çš„ä»£ç éƒ½æ˜¯æ¥è‡ªäºè¿™é‡Œ

[https://leimao.github.io/blog/Byte-Pair-Encoding/](https://leimao.github.io/blog/Byte-Pair-Encoding/)

```python
'''
https://leimao.github.io/blog/Byte-Pair-Encoding/
'''

import re, collections

def get_vocab(filename):
    vocab = collections.defaultdict(int)
    with open(filename, 'r', encoding='utf-8') as fhand:
        for line in fhand:
            words = line.strip().split()
            for word in words:
                vocab[' '.join(list(word)) + ' </w>'] += 1

    return vocab

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

def get_tokens_from_vocab(vocab):
    tokens_frequencies = collections.defaultdict(int)
    vocab_tokenization = {}
    for word, freq in vocab.items():
        word_tokens = word.split()
        for token in word_tokens:
            tokens_frequencies[token] += freq
        vocab_tokenization[''.join(word_tokens)] = word_tokens
    return tokens_frequencies, vocab_tokenization

def measure_token_length(token):
    if token[-4:] == '</w>':
        return len(token[:-4]) + 1
    else:
        return len(token)

def tokenize_word(string, sorted_tokens, unknown_token='</u>'):
    
    if string == '':
        return []
    if sorted_tokens == []:
        return [unknown_token]

    string_tokens = []
    for i in range(len(sorted_tokens)):
        token = sorted_tokens[i]
        token_reg = re.escape(token.replace('.', '[.]'))

        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]
        if len(matched_positions) == 0:
            continue
        substring_end_positions = [matched_position[0] for matched_position in matched_positions]

        substring_start_position = 0
        for substring_end_position in substring_end_positions:
            substring = string[substring_start_position:substring_end_position]
            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)
            string_tokens += [token]
            substring_start_position = substring_end_position + len(token)
        remaining_substring = string[substring_start_position:]
        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)
        break
    return string_tokens

# vocab = {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}

vocab = get_vocab('../data/pg16457.txt')

print('==========')
print('Tokens Before BPE')
tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)
print('All tokens: {}'.format(tokens_frequencies.keys()))
print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))
print('==========')

num_merges = 10000
for i in range(num_merges):
    pairs = get_stats(vocab)
    if not pairs:
        break
    best = max(pairs, key=pairs.get)
    vocab = merge_vocab(best, vocab)
    print('Iter: {}'.format(i))
    print('Best pair: {}'.format(best))
    tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)
    print('All tokens: {}'.format(tokens_frequencies.keys()))
    print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))
    print('==========')

# Let's check how tokenization will be for a known word
word_given_known = 'mountains</w>'
word_given_unknown = 'Ilikeeatingapples!</w>'

sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)
sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]

print(sorted_tokens)

word_given = word_given_known 

print('Tokenizing word: {}...'.format(word_given))
if word_given in vocab_tokenization:
    print('Tokenization of the known word:')
    print(vocab_tokenization[word_given])
    print('Tokenization treating the known word as unknown:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))
else:
    print('Tokenizating of the unknown word:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))

word_given = word_given_unknown 

print('Tokenizing word: {}...'.format(word_given))
if word_given in vocab_tokenization:
    print('Tokenization of the known word:')
    print(vocab_tokenization[word_given])
    print('Tokenization treating the known word as unknown:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))
else:
    print('Tokenizating of the unknown word:')
    print(tokenize_word(string=word_given, sorted_tokens=sorted_tokens, unknown_token='</u>'))

'''
Tokenizing word: mountains</w>...
Tokenization of the known word:
['mountains</w>']
Tokenization treating the known word as unknown:
['mountains</w>']
Tokenizing word: Ilikeeatingapples!</w>...
Tokenizating of the unknown word:
['I', 'like', 'ea', 'ting', 'app', 'l', 'es!</w>']
'''
```



## BBPE

å¯¹äºè‹±æ–‡æ¥è¯´ï¼Œä½¿ç”¨BPEèƒ½å¤Ÿåœ¨è¯è¡¨å¯æ§çš„å‰æä¸‹è§£å†³OOVçš„é—®é¢˜ï¼Œä½†æ˜¯å¯¹äºä¸­æ–‡ã€æ—¥æ–‡ç­‰è¯­è¨€æ—¶ï¼Œç¨€æœ‰æ–‡å­—å¯èƒ½ä¼šä¸å¿…è¦çš„å ç”¨è¯è¡¨å¤§å°ã€‚æ‰€ä»¥BBPEæ–¹æ³•æ˜¯å°†ä¸€æœ¬æ–‡æœ¬çš„UTF-8ç¼–ç ä¸­çš„ä¸€ä¸ªå­—èŠ‚256ä½ä¸åŒç¼–ç ä½œä¸ºè¯è¡¨çš„åˆå§‹åŒ–Subwordã€‚

ç›¸æ¯”ASCIIåªèƒ½è¦†ç›–è‹±æ–‡ä¸­å­—ç¬¦ï¼Œ**UTF-8ç¼–ç åˆ›å»ºçš„æœ¬èº«å°±æ˜¯ä¸ºäº†é€šç”¨çš„å°†ä¸–ç•Œä¸Šä¸åŒçš„è¯­è¨€å­—ç¬¦å°½å¯èƒ½å…¨éƒ¨ç”¨ä¸€å¥—ç¼–ç è¿›è¡Œç¼–å·**ï¼ŒåŒæ—¶ç›¸æ¯”UTF-32å¯¹äºæ¯ä¸ªå­—ç¬¦éƒ½é‡‡ç”¨4ä½å­—èŠ‚ï¼ˆbyteï¼‰è¿‡äºå†—é•¿ã€‚æ”¹è¿›çš„UTF-8ç¼–ç æ˜¯ä¸€ä¸ªå˜é•¿çš„ç¼–ç ï¼Œæœ‰1ï½4ä¸ªèŒƒå›´çš„å­—èŠ‚(bytes)é•¿åº¦ã€‚å¯¹äºä¸åŒè¯­è¨€ä¸­å­—ç¬¦é‡‡ç”¨ä¸åŒé•¿åº¦çš„å­—èŠ‚ç¼–ç ï¼Œä¾‹å¦‚è‹±æ–‡å­—ç¬¦åŸºæœ¬éƒ½æ˜¯1ä¸ªå­—èŠ‚ï¼ˆbyteï¼‰ï¼Œä¸­æ–‡æ±‰å­—é€šå¸¸éœ€è¦2ï½3ä¸ªå­—èŠ‚ã€‚

æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨byteæ¥æ„å»ºæœ€åŸºç¡€çš„è¯è¡¨è€Œä¸æ˜¯å­—ç¬¦ã€‚é¦–å…ˆå°†æ–‡æœ¬æŒ‰ç…§UTF-8è¿›è¡Œç¼–ç ï¼Œæ¯ä¸ªå­—ç¬¦åœ¨UTF-8çš„è¡¨ç¤ºä¸­å æ®1-4ä¸ªbyteã€‚ åœ¨byteåºåˆ—ä¸Šå†ä½¿ç”¨BPEç®—æ³•ï¼Œè¿›è¡Œbyte levelçš„ç›¸é‚»åˆå¹¶ã€‚ç¼–ç å½¢å¼å¦‚ä¸‹ã€‚

![](/images/notion_59f77253-3380-41e6-b663-3c6954cc0aab44df21ab-260b-49d9-b8fb-021817534d3c.png)

åœ¨è§£ç é˜¶æ®µï¼Œä¸€ä¸ªbyteåºåˆ—å¯èƒ½è§£ç åä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„å­—ç¬¦åºåˆ—ï¼Œè¿™é‡Œéœ€è¦åŠ¨æ€è§„åˆ’è¿›è¡Œè§£ç ï¼Œä½¿å…¶èƒ½è§£ç å‡ºå°½å¯èƒ½å¤šçš„åˆæ³•å­—ç¬¦ã€‚æ‰€ä»¥BBPEè™½ç„¶è§£å†³äº†ä¸­æ–‡ã€æ—¥æ–‡è¯è¡¨è¿‡å¤§çš„é—®é¢˜ï¼Œä½†æ˜¯è§£ç ä¸Šè¿˜æ˜¯æœ‰ä¸€äº›é—®é¢˜ã€‚

## **WordPiece**

> ğŸ’¡ è¾“å…¥ï¼šè®­ç»ƒè¯­æ–™ï¼› è¯è¡¨å¤§å°V

1.å‡†å¤‡è¶³å¤Ÿå¤§çš„è®­ç»ƒè¯­æ–™ï¼Œç¡®å®šæœŸæœ›çš„subwordè¯è¡¨å¤§å°ï¼›
2.å‡†å¤‡åŸºç¡€è¯è¡¨ï¼šæ¯”å¦‚è‹±æ–‡ä¸­26ä¸ªå­—æ¯åŠ ä¸Šå„ç§ç¬¦å·ï¼›
3.åŸºäºåŸºç¡€è¯è¡¨å°†è¯­æ–™ä¸­çš„å•è¯æ‹†åˆ†ä¸ºæœ€å°å•å…ƒ;
4.åŸºäºç¬¬3æ­¥æ•°æ®è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥æ˜¯æœ€ç®€å•çš„unigramè¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æå¤§ä¼¼ç„¶è¿›è¡Œä¼°è®¡å³å¯;
5.ä»æ‰€æœ‰å¯èƒ½çš„subwordå•å…ƒä¸­é€‰æ‹©åŠ å…¥è¯­è¨€æ¨¡å‹åèƒ½æœ€å¤§ç¨‹åº¦åœ°å¢åŠ è®­ç»ƒæ•°æ®æ¦‚ç‡çš„å•å…ƒä½œä¸ºæ–°çš„å•å…ƒ;
6.é‡å¤ç¬¬5æ­¥ç›´åˆ°è¾¾åˆ°ç¬¬2æ­¥è®¾å®šçš„subwordè¯è¡¨å¤§å°æˆ–æ¦‚ç‡å¢é‡ä½äºæŸä¸€é˜ˆå€¼;

WordPieceç®—æ³•è·ŸBPEç®—æ³•æµç¨‹åŸºæœ¬ä¸€è‡´ï¼ŒBPEæ˜¯åŸºäºæœ€é«˜è¯é¢‘æ¥ç”ŸæˆSubwordï¼Œè€ŒWordPieceæ˜¯åŸºäºæ¦‚ç‡æ¥ç”ŸæˆSubwordã€‚å…·ä½“æ¥è¯´ï¼Œæœ‰ä¸€ä¸ªè¯ä¸ºzï¼Œzç”±xå’Œyç»„æˆï¼Œå¯¹æ¯”P(Z)çš„æ¦‚ç‡å’ŒP(X)+P(Y)çš„æ¦‚ç‡ï¼Œå¦‚æœP(Z)>P(X)+P(Y)ï¼Œåˆ™åˆå¹¶xå’Œyï¼Œå› ä¸ºä»–ä»¬å…·æœ‰æœ€çš„äº’ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯åœ¨è¯­è¨€æ¨¡å‹ä¸Šå…·æœ‰è¾ƒå¼ºçš„å…³è”æ€§ã€‚

æˆ‘ä»¬å‡è®¾å¥å­S=(t_1,t_2,...,t_n)ï¼Œå…¶ä¸­t_iä»£è¡¨ä¸€ä¸ªå­è¯ï¼Œä¸”å‡è®¾å„ä¸ªå­è¯ä¹‹é—´æ˜¯ç›¸äº’ç‹¬ç«‹å­˜åœ¨çš„ï¼Œé‚£ä¹ˆå¥å­çš„è¯­è¨€æ¨¡å‹ä¼¼ç„¶å€¼ç­‰ä»·äºæ‰€æœ‰å­è¯æ¦‚ç‡çš„ä¹˜ç§¯ã€‚

å‡è®¾æŠŠç›¸é‚»ä½ç½®çš„xå’Œyä¸¤ä¸ªå­è¯è¿›è¡Œåˆå¹¶ï¼Œåˆå¹¶åäº§ç”Ÿçš„æ–°çš„å­—è¯ä¸ºzï¼Œæ­¤æ—¶å¥å­Sçš„ä¼¼ç„¶å€¼å˜åŒ–å¯ä»¥è¡¨ç¤ºä¸ºï¼š

ä»ä¸Šé¢çš„å…¬å¼å¯ä»¥å‘ç°ï¼Œä¼¼ç„¶å€¼çš„å˜åŒ–å°±æ˜¯ä¸¤ä¸ªå­è¯çš„äº’ä¿¡æ¯ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒWordPieceæ¯æ¬¡é€‰æ‹©åˆå¹¶çš„ä¸¤ä¸ªå­è¯ï¼Œä»–ä»¬å…·æœ‰æœ€å¤§çš„äº’ä¿¡æ¯å€¼ï¼Œä¹Ÿå°±æ˜¯ç›¸å…³å­—è¯åœ¨è¯­è¨€æ¨¡å‹ä¸Šå…·æœ‰è¾ƒå¼ºçš„å…³è”æ€§ï¼Œä»–ä»¬ç»å¸¸åœ¨è¯­æ–™ä¸­ä»¥ç›¸é‚»çš„æ–¹å¼åŒæ—¶å‡ºç°ã€‚

## UniLM

> ğŸ’¡ è¾“å…¥ï¼šè®­ç»ƒè¯­æ–™ï¼›è¯è¡¨å¤§å°Vï¼› ä¿ç•™é˜ˆå€¼Xï¼›

1.å‡†å¤‡åŸºç¡€è¯è¡¨ï¼šåˆå§‹åŒ–ä¸€ä¸ªå¾ˆå¤§çš„è¯è¡¨ï¼Œæ¯”å¦‚æ‰€æœ‰å­—ç¬¦+é«˜é¢‘Ngramï¼Œä¹Ÿå¯ä»¥é€šè¿‡BPEç®—æ³•åˆå§‹åŒ–ï¼›
2.é’ˆå¯¹å½“å‰è¯è¡¨ï¼Œç”¨è¯­è¨€æ¨¡å‹ï¼ˆunigram lm)ä¼°è®¡æ¯ä¸ªå­è¯åœ¨è¯­æ–™ä¸Šçš„æ¦‚ç‡ï¼›
3.è®¡ç®—åˆ é™¤æ¯ä¸ªSubwordåå¯¹æ€»lossçš„å½±å“åŠScoreå¾—åˆ†ï¼Œä½œä¸ºè¯¥Subwordæ’åºçš„Scoreå¾—åˆ†ï¼›
4.å°†å­è¯æŒ‰ç…§Scoreå¤§å°è¿›è¡Œæ’åºï¼Œä¿ç•™å‰X%çš„Subwordï¼›æ³¨æ„ï¼Œå»ºè®®å•å­—ç¬¦çš„Subwordä¸èƒ½è¢«ä¸¢å¼ƒï¼Œä»¥å…OOVï¼›
5.é‡å¤æ­¥éª¤2åˆ°4ï¼Œç›´åˆ°è¯è¡¨å¤§å°å‡å°‘åˆ°è®¾å®šå€¼ï¼›

UniLMå¯ä»¥çœ‹æˆæ˜¯WordPieceç®—æ³•åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­è¿›è¡Œåå‘æ“ä½œã€‚ç›¸æ¯”WordPieceå€ŸåŠ©è¯­è¨€æ¨¡å‹é€‰æ‹©åˆå¹¶è®¡ç®—æ¦‚ç‡æœ€å¤§çš„ç›¸é‚»å­—ç¬¦å¯¹åŠ å…¥è¯è¡¨ä¸­ä½œä¸ºæ–°çš„Subwordï¼ŒUniLMæ˜¯å¼€å§‹æ—¶å…ˆæ„å»ºè¶³å¤Ÿå¤§çš„è¯è¡¨ï¼Œä¹‹åæ¯ä¸€æ­¥é€‰æ‹©ä¸€å®šæ¯”ä¾‹çš„è®¡ç®—æ¦‚ç‡ä½çš„Subwordä»è¯è¡¨ä¸­åˆ é™¤ã€‚å› æ­¤è¿‡ç¨‹ä¸­æ¯”è¾ƒæ˜¾è‘—å·®åˆ«æ˜¯WordPieceç®—æ³•çš„è¯è¡¨åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¯ä»å°åˆ°å¤§çš„å˜åŒ–ï¼Œè€ŒUniLMçš„è¯è¡¨åˆ™æ˜¯ä»å¤§åˆ°å°çš„å˜åŒ–ï¼Œæ•´ä¸ªè¿‡ç¨‹æ ¹æ®è¯„ä¼°ä¸æ–­åˆ é™¤æ’åºé åçš„Subwordç›´åˆ°è¯è¡¨å¤§å°å‡å°‘åˆ°è®¾å®šå€¼ã€‚

## **SentencePiece**

SentencePieceæ˜¯Googleæ¨å‡ºçš„åˆ†è¯å·¥å…·ï¼Œè¿™ä¸ªåŒ…ä¸»è¦æ˜¯ä¸ºäº†å¤šè¯­è¨€æ¨¡å‹è®¾è®¡çš„ï¼Œä½¿ç”¨unicodeç¼–ç ï¼Œè§£å†³äº†å¤šè¯­è¨€ç¼–ç æ–¹å¼ä¸åŒçš„é—®é¢˜ã€‚

- å†…ç½®BPEï¼ŒUnigramï¼Œcharå’Œwordçš„åˆ†è¯æ–¹æ³•
- æ— éœ€é¢„åˆ†è¯ï¼Œä»¥unicodeæ–¹å¼ç›´æ¥ç¼–ç æ•´ä¸ªå¥å­ï¼Œç©ºæ ¼ä¼šè¢«ç‰¹æ®Šç¼–ç ä¸ºâ–
- ç›¸æ¯”ä¼ ç»Ÿå®ç°è¿›è¡Œä¼˜åŒ–ï¼Œåˆ†è¯é€Ÿåº¦é€Ÿåº¦æ›´å¿«
ä¸­æ–‡llamaæ¨¡å‹ä½¿ç”¨çš„å°±æ˜¯SentencePieceè¿›è¡Œæ‰©å……ä¸­æ–‡è¯è¡¨ã€‚å…·ä½“æµç¨‹ä¸ºï¼šå…ˆä½¿ç”¨å¤§é‡çš„ä¸­æ–‡é¢„æ–™è¿›è¡Œè®­ç»ƒä¸­æ–‡tokenizeræ¨¡å‹ï¼Œç„¶åå°†ä¸­æ–‡æ¨¡å‹ä¸llama tokenizeræ¨¡å‹è¿›è¡Œåˆå¹¶ï¼Œåˆå¹¶ä»£ç ï¼š

[https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_tokenizer/merge_tokenizers.py](https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_tokenizer/merge_tokenizers.py)

## å‚è€ƒæ–‡ç« 

[https://mp.weixin.qq.com/s/Sgz74bSs0saCFhw1GOaMnw](https://mp.weixin.qq.com/s/Sgz74bSs0saCFhw1GOaMnw)

[https://zhuanlan.zhihu.com/p/86965595](https://zhuanlan.zhihu.com/p/86965595)

[https://zhuanlan.zhihu.com/p/649030161](https://zhuanlan.zhihu.com/p/649030161)

[https://zhuanlan.zhihu.com/p/651430181](https://zhuanlan.zhihu.com/p/651430181)

[https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary)



