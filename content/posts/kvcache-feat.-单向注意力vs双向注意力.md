---
{
  "title": "KVCache feat. å•å‘æ³¨æ„åŠ›VSåŒå‘æ³¨æ„åŠ›",
  "date": "2024-06-12",
  "tags": [
    "LLM inference"
  ],
  "categories": [
    "LLM"
  ],
  "summary": "",
  "authors": [],
  "draft": false,
  "generated_time": "2025-02-24T12:15:11.815204"
}
---

## å‰è¨€

åœ¨ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ¨ç†æ—¶æˆ‘ä»¬éƒ½ä¼šè§‰å¾—æ¨¡å‹çš„æ¨ç†é€Ÿåº¦æ¯”è¾ƒæ…¢ï¼Œå½“å‰ä¹Ÿæœ‰å¾ˆå¤šæ¨ç†åŠ é€Ÿçš„æ–¹æ³•ï¼Œåƒæ˜¯æ¨¡å‹é‡åŒ–ã€FlashAttentionã€ä½¿ç”¨æ›´å¥½çš„GPUã€KVCacheç­‰ï¼Œä»Šå¤©æˆ‘ä»¬å°±æ¥èŠèŠKVCacheæ˜¯æ€ä¹ˆå›äº‹ï¼Œæˆ‘ä¼šä»transformer decoderçš„æ¨ç†è¿‡ç¨‹å¼€å§‹ï¼Œåˆ°ä¸ºä»€ä¹ˆKVCacheå¯ä»¥åŠ é€Ÿæ¨¡å‹æ¨ç†ï¼Œä»¥åŠå…·ä½“çš„æ•ˆæœè¿›è¡Œä»‹ç»ã€‚

## Transformerçš„self-attentionæœºåˆ¶

ç›®å‰çš„å¤§æ¨¡å‹éƒ½æ˜¯åŸºäºTransformerçš„decoderæ¶æ„è¿›è¡Œè®¾è®¡çš„ï¼Œæ‰€ä»¥å…ˆæŠŠè‘—åçš„self-attentionå…¬å¼æŠ›å‡ºæ¥ã€‚

ç”±äºå¤§æ¨¡å‹ä½¿ç”¨çš„æ˜¯decoderï¼Œæ‰€ä»¥åœ¨è®¡ç®—æ—¶ä¼šmaskæ‰å½“å‰tokençš„åè¾¹çš„tokenï¼Œä¾‹å¦‚å½“å‰ä¸ºtoken_nï¼ŒmaskçŸ©é˜µä¼šmaskæ‰nä¹‹åçš„æ‰€æœ‰tokenå€¼ï¼Œç›®çš„æ˜¯é˜²æ­¢è§£ç è¿‡ç¨‹ä¸­çœ‹åˆ°ç­”æ¡ˆã€‚

ä¸‹å›¾æ˜¯GPT2çš„è§£ç è¿‡ç¨‹ï¼Œç»™å®šè¾“å…¥ï¼Œæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œç„¶ååœ¨ä¸‹ä¸€æ­¥ä¸­ä½¿ç”¨ä¸Šä¸€æ­¥é¢„æµ‹çš„tokenä½œä¸ºè¾“å…¥å†æ¬¡è¿›è¡Œé¢„æµ‹ã€‚å›¾æ¥æºï¼š[https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)

![](/images/notion_4057612f-0c72-4a90-a7ee-7b28e0d726dc56e50bbd-2997-4f89-bfeb-a27377d1c666.gif)

æˆ‘ä»¬æ¥é€æ­¥è®¡ç®—ä¸€ä¸‹è§£ç çš„è¿‡ç¨‹ã€‚

å› ä¸ºdecoderä¸­çš„self-attentionæ˜¯masked self-attentionï¼Œæ‰€ä»¥åœ¨è®¡ç®—æ—¶éœ€è¦æ³¨æ„è¿›è¡Œmaskã€‚

æˆ‘ä»¬å°†qå’Œkçš„è®¡ç®—æ•´ç†æˆçŸ©é˜µï¼Œå¯ä»¥å¾—åˆ°ä»¥ä¸‹å…¬å¼ã€‚

å±•å¼€çš„è¯å°±æ˜¯è¿™æ ·çš„ä¸€ä¸ªçŸ©é˜µã€‚

å¯¹äºtoken_1ï¼ŒAttention_1(Q,K,V)=softmax(\frac{Q_1K^T_1}{\sqrt{d_k}})\vec{V_1}

å¯¹äºtoken_2ï¼ŒAttention_2(Q,K,V)=softmax(\frac{Q_2K^T_1}{\sqrt{d_k}})\vec{V_1}+softmax(\frac{Q_2K^T_2}{\sqrt{d_k}})\vec{V_2}

å¯¹äºtoken_3ï¼ŒAttention_3(Q,K,V)=softmax(\frac{Q_3K^T_1}{\sqrt{d_k}})\vec{V_1}+softmax(\frac{Q_3K^T_2}{\sqrt{d_k}})\vec{V_2}+softmax(\frac{Q_3K^T_3}{\sqrt{d_k}})\vec{V_3}

å¯ä»¥çœ‹åˆ°åœ¨token_2æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼ŒK_1ã€V_1æ˜¯é‡å¤ä½¿ç”¨çš„ï¼›å¯ä»¥çœ‹åˆ°åœ¨token_3æ¨ç†çš„è¿‡ç¨‹ä¸­ï¼ŒK_1ã€V_1ã€K_2ã€V_2æ˜¯é‡å¤ä½¿ç”¨çš„ã€‚

è¿˜å¯ä»¥çœ‹å‡ºæ¥ï¼Œæ¯æ¬¡è®¡ç®—attentionåªéœ€è¦ä½¿ç”¨å½“å‰çš„Qå³å¯ï¼Œå¹¶ä¸éœ€è¦ä¹‹å‰çš„å‘é‡ã€‚

æ‰€ä»¥æˆ‘ä»¬å°±å¯ä»¥æŠŠä¹‹å‰è®¡ç®—è¿‡çš„Kã€Vç¼“å­˜èµ·æ¥ï¼Œè¿™å°±æ˜¯ä»Šå¤©æˆ‘ä»¬è¦ä»‹ç»çš„KVCacheã€‚

## KVCacheçš„ä½œç”¨

ç°åœ¨æˆ‘ä»¬å°±å¯ä»¥ç»™KVCacheè¿›è¡Œå®šä¹‰äº†ï¼Œåœ¨decoder-onlyæ¶æ„ä¸­ï¼Œé€šè¿‡ç¼“å­˜è§£ç è¿‡ç¨‹ä¸­çš„Kã€Vï¼Œæ¥é¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œè¾¾åˆ°æ¨ç†åŠ é€Ÿçš„æ•ˆæœã€‚

ä¸‹å›¾æ¸…æ™°äº†å¯¹æ¯”äº†ä½¿ç”¨KVCacheå’Œä¸ä½¿ç”¨KVCacheçš„åŒºåˆ«ï¼Œå›¾æ¥æº[https://medium.com/@joaolages/kv-caching-explained-276520203249](https://medium.com/@joaolages/kv-caching-explained-276520203249)

![](/images/notion_2772e1c2-6780-4c33-a6aa-75a1b091219d9109d308-9ff2-4e32-8ffd-f1fd908c98a1.gif)

## å®ç°ç»†èŠ‚

åœ¨huggingfaceçš„transformersä¸­ï¼Œé€šè¿‡modelling_gpt2.pyå¯ä»¥çœ‹åˆ°å…·ä½“çš„å®ç°ç»†èŠ‚ã€‚

```python
        query = self._split_heads(query, self.num_heads, self.head_dim)
        key = self._split_heads(key, self.num_heads, self.head_dim)
        value = self._split_heads(value, self.num_heads, self.head_dim)

				# é€šè¿‡layer_pastæ¥ä¼ é€’ä¸Šä¸€æ¬¡è®¡ç®—çš„key value
        if layer_past is not None:
            past_key, past_value = layer_past
            # é€šè¿‡æ‹¼æ¥æ¥å¾—åˆ°æœ€æ–°çš„key å’Œ value
            key = torch.cat((past_key, key), dim=-2)
            value = torch.cat((past_value, value), dim=-2)

        if use_cache is True:
            present = (key, value)
        else:
            present = None
```

## åœ¨transformersä¸­ä½¿ç”¨KVCacheå¯¹æ¯”

åœ¨Transformersä¸­è¿›è¡Œå¯¹æ¯”å¼€å¯KVCacheåçš„æ•ˆæœã€‚

```python
import numpy as np
import time
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2").to(device)

for use_cache in (True, False):
  times = []
  for _ in range(10):  # measuring 10 generations
    start = time.time()
    model.generate(**tokenizer("What is KV caching?", return_tensors="pt").to(device), use_cache=use_cache, max_new_tokens=1000)
    times.append(time.time() - start)
  print(f"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds")
```

å› ä¸ºæˆ‘æ˜¯è·‘åœ¨M1ä¸Šï¼Œæ‰€ä»¥å¯¹æ¯”æ¯”è¾ƒæ˜æ˜¾ï¼Œå¼€å¯KVCacheåæ¯”ä¸å¼€å¯èƒ½å¿«100å€ã€‚

```plain text
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
with KV caching: 32.101 +- 0.737 seconds
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
without KV caching: 3516.684 +- 27.119 seconds
```

## MHA MQA GQA

ç›®å‰å¾ˆå¤šæ¨¡å‹ï¼Œåƒæ˜¯LLaMAã€Qwenï¼Œéƒ½ä½¿ç”¨GQAï¼Œå…¶ç›®çš„ä¹Ÿæ˜¯ä¸ºäº†æé«˜æ¨¡å‹è¿è¡Œçš„é€Ÿåº¦ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä»KVCacheçš„è§’åº¦å»ç†è§£è¿™äº›æ–¹æ³•ï¼Œä»–ä»¬çš„ç›®çš„å…¶å®å°±æ˜¯ä¸ºäº†å‡å°‘KVçš„ä¸ªæ•°ï¼Œä»è€Œå‡å°‘KVCacheã€‚

![](/images/notion_647e6c69-0838-4b7b-aad4-c778b41354af6af07664-95f7-428f-8e34-360fca53a1a9.webp)

MHAï¼ˆMulti-Head Attentionï¼‰ï¼Œä¼ ç»Ÿçš„Transformerçš„attentionæœºåˆ¶ï¼Œä¹Ÿå°±æ˜¯å¯¹queryã€keyã€valueåˆ†åˆ«ä½¿ç”¨å•ç‹¬çš„å¤´ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹å¤„ç†è¾“å…¥çš„ä¸åŒæ–¹é¢ï¼Œä¼˜ç‚¹å°±æ˜¯ç‹¬ç«‹è®¡ç®—ï¼Œæ•ˆæœæœ€å¥½ï¼Œä½†æ˜¯è®¡ç®—æˆæœ¬å¤ªé«˜ã€‚

MQAï¼ˆMulti-Query Attentionï¼‰ï¼Œæ‰€æœ‰çš„queryã€keyã€valueä½¿ç”¨ä¸€ä¸ªå¤´ï¼Œè¿™æ ·å°±å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œæ¯”MHAå¿«çš„å¤šï¼Œä½†æ˜¯ä¼šå½±å“æ¨¡å‹çš„æ•ˆæœã€‚

GQAï¼ˆGrouped-Query Attentionï¼‰ï¼Œæ˜¯MHAå’ŒMQAçš„ä¸­é—´åœ°å¸¦ã€‚å°†å¤šä¸ªå¤´è¿›è¡Œåˆ†ç»„ï¼Œæ¯ä¸ªç»„å†…å…±äº«ä¸€ä¸ªkeyå’Œvalueï¼Œè¿™æ ·æ¯ä¸ªç»„çš„å¤´æ•°è¾ƒå°‘ï¼Œæ‰€ä»¥æ¯”MHAå¿«ï¼Œå¹¶ä¸”å¤´æ•°ä¸æ˜¯ä¸€ä¸ªå¤´ï¼Œæ‰€ä»¥æ•ˆæœè¦æ¯”MQAè¦å¥½ã€‚

## æ˜¾å­˜åˆ†æ

å‡è®¾è¾“å…¥çš„åºåˆ—é•¿åº¦æ˜¯ ğ‘šï¼Œè¾“å‡ºåºåˆ—é•¿åº¦æ˜¯ ğ‘› ï¼Œ ğ‘ ä¸ºæ•°æ®æ‰¹æ¬¡å¤§å°ï¼Œ ğ‘™ ä¸ºå±‚æ•°ï¼Œ â„ ä¸ºéšå‘é‡ç»´åº¦ï¼Œä»¥ FP16ï¼ˆ2bytesï¼‰ æ¥ä¿å­˜ï¼Œé‚£ä¹ˆ KVCacheçš„å³°å€¼æ˜¾å­˜å ç”¨å¤§å°ä¸º ğ‘(ğ‘š+ğ‘›)â„âˆ—ğ‘™âˆ—2âˆ—2=4ğ‘ğ‘™â„(ğ‘š+ğ‘›)Â ï¼Œç¬¬ä¸€ä¸ª 2 ä»£è¡¨ Kã€Vï¼Œç¬¬äºŒä¸ª 2 ä»£è¡¨ 2bytesã€‚å¯è§éšç€æ‰¹æ¬¡å¤§å°å’Œé•¿åº¦çš„å¢åŠ ï¼ŒKVCache çš„æ˜¾å­˜å ç”¨ä¹Ÿä¼šå¿«é€Ÿå¢å¤§ã€‚

æ‰€ä»¥KVCacheçš„æ˜¾å­˜å ç”¨è·Ÿåºåˆ—é•¿åº¦æ˜¯æ­£ç›¸å…³çš„ï¼Œç°åœ¨æ¨¡å‹åŠ¨ä¸åŠ¨å°±æ˜¯4kä¸Šä¸‹æ–‡ï¼Œç”šè‡³ä¸Šç™¾ä¸‡ä¸Šä¸‹æ–‡ï¼Œè¿™å¯¹GPUçš„æ˜¾å­˜å ç”¨æ˜¯å¾ˆå¤§çš„ï¼Œå…·ä½“è§£å†³çš„æ–¹æ³•æœ‰ä»¥ä¸‹ä¸‰ä¸ªã€‚

1. åˆ†é…ä¸€ä¸ªæœ€å¤§å®¹é‡çš„ç¼“å†²åŒºï¼Œè¦æ±‚æå‰é¢„çŸ¥æœ€å¤§çš„tokenæ•°é‡ã€‚å¦‚æœç”¨æˆ·çš„ä¸Šä¸‹æ–‡å¾ˆçŸ­çš„èŠ±ï¼Œè¿™æ ·ä¼šå¾ˆæµªè´¹èµ„æºã€‚
1. åŠ¨æ€åˆ†é…ç¼“å†²åŒºï¼Œå…ˆè®¾ç½®å›ºå®šçš„å®¹é‡ï¼Œè¶…è¿‡äº†å°±è¿›è¡Œæ‰©å®¹å¤„ç†ï¼Œä½†æ˜¯åœ¨åœ¨GPUä¸Šé¢‘ç¹ç”³è¯·ã€é‡Šæ”¾å†…å­˜çš„å¼€é”€æ˜¯å¾ˆå¤§çš„ï¼Œæ•ˆç‡ä¸å¤Ÿé«˜ã€‚
1. ä¸æ•°æ®æ‹†æ•£ï¼ŒæŒ‰æœ€å°å•å…ƒå­˜å‚¨ï¼Œä½¿ç”¨ä¸€ä¸ªå…ƒæ•°æ®è®°å½•æ¯ä¸€ä¸ªæ•°æ®çš„ä½ç½®ã€‚è¿™å°±æ˜¯å¤§åé¼é¼çš„PagedAttentionï¼Œä¹Ÿå°±æ˜¯vLLMçš„ä¸»è¦æŠ€æœ¯ã€‚
## å•å‘æ³¨æ„åŠ›VSåŒå‘æ³¨æ„åŠ›

æœ€åæˆ‘ä»¬æ¥èŠèŠä¸ºä»€ä¹ˆLLMéƒ½æ˜¯ä½¿ç”¨decoder-onlyçš„æ¶æ„ï¼Œä¹Ÿå°±æ˜¯å•å‘æ³¨æ„åŠ›æœºåˆ¶ã€‚

åœ¨GPT3ä¹‹å‰ï¼ŒBERTåœ¨NLPé¢†åŸŸæ˜¯ç»å¯¹çš„éœ¸ä¸»ï¼Œå‡ ä¹æ‰€æœ‰çš„ä»»åŠ¡éƒ½ä¼šä½¿ç”¨BERTå»åšï¼Œéƒ½èƒ½è¾¾åˆ°SOTAçš„æ°´å¹³ï¼ŒBERTä½¿ç”¨çš„æ˜¯Transformerçš„encoderæ¶æ„ï¼Œä¹Ÿå°±æ˜¯åŒå‘æ³¨æ„åŠ›æœºåˆ¶ã€‚åœ¨GPT3ä¹‹åï¼Œæˆ‘ä»¬è§è¯†åˆ°äº†å¤§åŠ›å‡ºå¥‡è¿¹ï¼Œå°¤å…¶æ˜¯ChatGPTçˆ†ç«ä»¥åï¼Œæˆ‘ä»¬è§è¯†åˆ°äº†decoder-onlyçš„æ¶æ„ï¼Œä¹Ÿå°±æ˜¯å•å‘æ³¨æ„åŠ›çš„å¨åŠ›ã€‚é‚£ä¸ºä»€ä¹ˆå•å‘æ³¨æ„åŠ›åŠ ä¸Šè¶…å¤§è§„æ¨¡çš„é¢„è®­ç»ƒåæ•ˆæœè¿™ä¹ˆå¥½å‘¢ï¼Ÿ

è‹å‰‘æ—è€å¸ˆçš„åšå®¢ä¸­åšäº†ç®€å•çš„å®éªŒï¼Œè¯æ˜äº†â€œè¾“å…¥éƒ¨åˆ†çš„æ³¨æ„åŠ›æ”¹ä¸ºåŒå‘ä¸ä¼šå¸¦æ¥æ”¶ç›Šï¼ŒEncoder-Decoderæ¶æ„çš„ä¼˜åŠ¿å¾ˆå¯èƒ½åªæ˜¯æºäºå‚æ•°ç¿»å€ã€‚â€ä¹Ÿå°±æ˜¯åœ¨åŒç­‰å‚æ•°é‡ã€åŒç­‰æ¨ç†æˆæœ¬ä¸‹ï¼Œdecoder-onlyçš„æ¶æ„æ˜¯æœ€ä¼˜é€‰æ‹©ã€‚

è¿˜æœ‰å°±æ˜¯å¤§å®¶éƒ½åœ¨è®¨è®ºçš„ä½ç§©é—®é¢˜ã€‚åœ¨è¾“å…¥éƒ¨åˆ†ä½¿ç”¨åŒå‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¾“å‡ºéƒ¨åˆ†ä½¿ç”¨å•å‘æ³¨æ„åŠ›æœºåˆ¶ï¼Œä¹Ÿå°±æ˜¯Prefix LMçš„åšæ³•ï¼Œç›´è§‰ä¸Šæ˜¯æœ€ä¼˜çš„é€‰æ‹©ï¼Œä½†æ˜¯åŒå‘æ³¨æ„åŠ›æœºåˆ¶çš„ä½ç§©é—®é¢˜ä¼šå¸¦æ¥æ•ˆæœä¸‹é™ã€‚å…·ä½“çš„ç»†èŠ‚å¤§å®¶å¯ä»¥å‚è€ƒè‹å‰‘æ—è€å¸ˆçš„åšå®¢ï¼Œæˆ‘å°±ä¸é‡å¤äº†ï¼Œå®åœ¨æ²¡æœ‰è‹å‰‘æ—è€å¸ˆå†™çš„ä¸“ä¸šã€‚

é¡ºç€è¿™ä¸ªé—®é¢˜ï¼Œåœ¨çŸ¥ä¹ä¸Šçœ‹åˆ°äº†ä¸€ç¯‡è®ºæ–‡ï¼Œã€ŠWhat Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?ã€‹ï¼Œè¿™ç¯‡è®ºæ–‡åœ¨50äº¿å‚æ•°å’Œ1700äº¿tokensä¸Šçš„é¢„è®­ç»ƒä»»åŠ¡åšäº†å¯¹æ¯”å®éªŒï¼Œå¾—å‡ºäº†ä»¥ä¸‹ç»“è®ºã€‚

1. å¦‚æœå¤§æ¨¡å‹åˆ¶ä½œæ— ç›‘ç£é¢„è®­ç»ƒï¼Œé‚£ä¹ˆdecoder-onlyæ¶æ„+NTPï¼ˆnext token predictionï¼‰ä»»åŠ¡çš„zero-shotæ³›åŒ–èƒ½åŠ›æœ€ä½³ã€‚
1. æ— ç›‘ç£é¢„è®­ç»ƒ+multitask finetuningåï¼Œencoder-decoderæ¶æ„+MLMï¼ˆmasked language modelingï¼‰ä»»åŠ¡çš„zero-shotæ³›åŒ–èƒ½åŠ›æœ€ä½³ã€‚
æ‰€ä»¥ï¼Œåœ¨åªä½¿ç”¨æ— ç›‘ç£é¢„è®­ç»ƒä»»åŠ¡çš„å‰æä¸‹ï¼Œè®©æ¨¡å‹å¯ä»¥å¤„ç†å¼€æ”¾æ€§é—®é¢˜å•å‘æ³¨æ„åŠ›æœºåˆ¶æ˜¯æœ€ä¼˜çš„é€‰æ‹©ã€‚è€Œæƒ³è¦åœ¨ç‰¹å®šçš„ä»»åŠ¡ä¸‹å¾—åˆ°æœ€ä¼˜å¯ä»¥é€‰æ‹©encoder-decoderæ¶æ„æˆ–è€…encoderæ¶æ„ï¼Œä½¿ç”¨æ— ç›‘ç£é¢„è®­ç»ƒ+finetuningã€‚

## é¢˜å¤–è¯

è¿™é‡Œç®€å•æä¸€ä¸‹decoder-onlyæ¶æ„çš„æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†promptçš„ï¼Œå‰è¾¹æˆ‘ä»¬æåˆ°è¿™ç§æ¶æ„çš„æ¨¡å‹åœ¨è¾“å‡ºçš„æ—¶å€™æ˜¯é€šè¿‡ä¸Šä¸€æ­¥ç”Ÿæˆçš„ç»“æœæ¥æ¨ç†ä¸‹ä¸€ä¸ªtokenï¼Œé‚£æˆ‘ä»¬è¾“å…¥çš„promptæ˜¯ä¸€æ•´å¥è¯ï¼Œæ¨¡å‹æ€ä¹ˆç¼–ç å‘¢ï¼Ÿç†è®ºä¸Šæ¥è¯´ä¹Ÿéœ€è¦é€šè¿‡ä¸Šä¸€ä¸ªtokené¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼Œä½†æ˜¯å› ä¸ºæˆ‘ä»¬è¾“å…¥çš„å¥å­æ˜¯ç¡®å®šçš„ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å·²ç»çŸ¥é“çš„promptçš„ä¸Šä¸‹æ–‡äº†ï¼Œæ‰€ä»¥æ²¡å¿…è¦é€šè¿‡ä¸Šä¸€ä¸ªtokenæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªtokenäº†ï¼Œå¹¶ä¸”æˆ‘è¿˜éœ€è¦å¯¹å…¶è¿›è¡Œç¼–ç ï¼Œé‚£å°±éœ€è¦maskçŸ©é˜µæ¥å¸®å¿™äº†ã€‚

é€šè¿‡ä¸Šè¾¹self-attentionçš„çŸ©é˜µå¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ä¸€ä¸ªä¸‹ä¸‰è§’çš„çŸ©é˜µï¼Œæ¥å®ç°å½“å‰tokençœ‹ä¸åˆ°åè¾¹tokençš„æ•ˆæœã€‚æ‰€ä»¥åœ¨å¤„ç†promptæ—¶ï¼Œæˆ‘ä»¬ä¼šç¼–ç æ•´ä¸ªå¥å­ï¼Œç„¶åé€šè¿‡ä¹˜ä¸Šè¿™æ ·çš„ä¸‹ä¸‰è§’çŸ©é˜µæ¥å®ç°ä¸€ä¸ªtokenä¸€ä¸ªtokenç¼–ç çš„æ•ˆæœã€‚

è¿™å°±å®ç°äº†promptå¹¶è¡Œå¤„ç†çš„æ•ˆæœï¼Œä»openAIçš„apiå®šä»·ä¸Šä¹Ÿèƒ½çœ‹å‡ºæ¥ï¼Œinputçš„ä»·æ ¼æ˜¯æ¯”outputçš„ä»·æ ¼è¦ä½çš„ï¼Œå°±æ˜¯å› ä¸ºinputå¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œæ¯”è¾ƒèŠ‚çœç®—åŠ›ã€‚

![](/images/notion_7f713a65-b661-4852-966e-28faa501acc2071e5b20-8c30-4a68-99b8-7d86cf3f9781.png)

## æ€»ç»“

æœ¬æ–‡ç®—æ˜¯æˆ‘çš„å­¦ä¹ ç¬”è®°ï¼Œä»ä»€ä¹ˆæ˜¯KVCacheï¼Œåˆ°å®ç°æ–¹æ³•åŠå…¶æ•ˆæœï¼Œå»¶ä¼¸åˆ°å•å‘æ³¨æ„åŠ›æœºåˆ¶çš„é—®é¢˜ï¼Œå‚è€ƒäº†å¤§é‡çš„åšå®¢ã€‚ç›®çš„æ˜¯è®©è‡ªå·±å¼„æ¸…æ¥šæ€ä¹ˆå›äº‹ï¼Œå¦‚æœèƒ½å¸®åŠ©åˆ°è¯»è€…é‚£å°†æ˜¯æˆ‘çš„è£å¹¸ï¼Œå¦‚æœæˆ‘å†™çš„ä¸å¤Ÿæ¸…æ¥šï¼Œæ¬¢è¿å¤§å®¶é˜…è¯»å‚è€ƒæ–‡çŒ®çš„åŸæ–‡ã€‚

## å‚è€ƒæ–‡çŒ®

[Transformers KV Caching Explained](https://medium.com/@joaolages/kv-caching-explained-276520203249)

[LLM Inference Series: 3. KV caching explained](https://medium.com/@plienhar/llm-inference-series-3-kv-caching-unveiled-048152e461c8)

[LLM Inference Series: 4. KV caching, a deeper look](https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8)

[æ¼«è°ˆæ³¨æ„åŠ›æœºåˆ¶ï¼ˆäº”ï¼‰ï¼šè‡ªæ³¨æ„åŠ›ä¸Transformer](https://allenwind.github.io/blog/9481/)

[ä¸€æ–‡è¯»æ‡‚KVCache](https://zhuanlan.zhihu.com/p/686183300)

[ä¸ºä»€ä¹ˆç°åœ¨çš„LLMéƒ½æ˜¯Decoder-onlyçš„æ¶æ„ï¼Ÿ](https://spaces.ac.cn/archives/9529)

ä¸ºä»€ä¹ˆç°åœ¨çš„LLMéƒ½æ˜¯Decoder onlyçš„æ¶æ„ï¼Ÿ - CastellanZhangçš„å›ç­” - çŸ¥ä¹
[https://www.zhihu.com/question/588325646/answer/3002928687](https://www.zhihu.com/question/588325646/answer/3002928687)



